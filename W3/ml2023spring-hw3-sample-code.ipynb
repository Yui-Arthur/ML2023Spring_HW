{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HW3 Image Classification\n#### Solve image classification with convolutional neural networks(CNN).\n#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com","metadata":{}},{"cell_type":"code","source":"# check GPU type.\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:48.133286Z","iopub.execute_input":"2023-03-15T11:41:48.133653Z","iopub.status.idle":"2023-03-15T11:41:49.145235Z","shell.execute_reply.started":"2023-03-15T11:41:48.133620Z","shell.execute_reply":"2023-03-15T11:41:49.144053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Packages","metadata":{}},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:49.149374Z","iopub.execute_input":"2023-03-15T11:41:49.149693Z","iopub.status.idle":"2023-03-15T11:41:49.157330Z","shell.execute_reply.started":"2023-03-15T11:41:49.149662Z","shell.execute_reply":"2023-03-15T11:41:49.156436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:49.158482Z","iopub.execute_input":"2023-03-15T11:41:49.158738Z","iopub.status.idle":"2023-03-15T11:41:51.645326Z","shell.execute_reply.started":"2023-03-15T11:41:49.158714Z","shell.execute_reply":"2023-03-15T11:41:51.644299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myseed = 1102  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:51.647734Z","iopub.execute_input":"2023-03-15T11:41:51.648790Z","iopub.status.idle":"2023-03-15T11:41:51.716306Z","shell.execute_reply.started":"2023-03-15T11:41:51.648750Z","shell.execute_reply":"2023-03-15T11:41:51.715330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\n\nimage_size = (256,256)\ntest_tfm = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize(image_size),\n    transforms.RandomRotation(30),\n    transforms.RandomCrop((256,256),padding_mode=\"edge\"),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomAdjustSharpness(1.5, p=0.5),\n    transforms.RandomAutocontrast(p=0.5),\n    transforms.RandomPosterize(4, p=0.5),\n    transforms.RandomPerspective(distortion_scale=0.15,p=0.5),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.5,scale=(0.01,0.02),value=(1,0,0)),\n    transforms.RandomErasing(p=0.5,scale=(0.01,0.02),value=(0,1,0)),\n    transforms.RandomErasing(p=0.5,scale=(0.01,0.02),value=(0,0,1)),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:51.720311Z","iopub.execute_input":"2023-03-15T11:41:51.721253Z","iopub.status.idle":"2023-03-15T11:41:51.730501Z","shell.execute_reply.started":"2023-03-15T11:41:51.721213Z","shell.execute_reply":"2023-03-15T11:41:51.728834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return im,label","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:51.732033Z","iopub.execute_input":"2023-03-15T11:41:51.732599Z","iopub.status.idle":"2023-03-15T11:41:51.742811Z","shell.execute_reply.started":"2023-03-15T11:41:51.732558Z","shell.execute_reply":"2023-03-15T11:41:51.741939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FoodDataset_TTA(Dataset):\n\n    def __init__(self,path, train_tfm , test_tfm , TTA_num , files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.train_transform = train_tfm\n        self.test_transform = test_tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        train_im = []\n        for i in range(TTA_num):\n             train_im.append(self.train_transform(im))\n            \n        test_im = self.test_transform(im)\n        \n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return train_im,test_im,label","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:51.744296Z","iopub.execute_input":"2023-03-15T11:41:51.744920Z","iopub.status.idle":"2023-03-15T11:41:51.756841Z","shell.execute_reply.started":"2023-03-15T11:41:51.744859Z","shell.execute_reply":"2023-03-15T11:41:51.756147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = models.efficientnet_v2_m()\n        self.fc = nn.Linear(1000 , 11)\n        \n    def forward(self, x):\n        out = self.cnn(x)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:51.758265Z","iopub.execute_input":"2023-03-15T11:41:51.759288Z","iopub.status.idle":"2023-03-15T11:41:51.766511Z","shell.execute_reply.started":"2023-03-15T11:41:51.759252Z","shell.execute_reply":"2023-03-15T11:41:51.765850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\nmodel = Classifier().to(device)\nbatch_size = 32\nn_epochs = 70\npatience = 5\ncriterion = nn.CrossEntropyLoss(label_smoothing = 0.1)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\nTTA_num = 5\nTTA_ratio = 0.8","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:51.768094Z","iopub.execute_input":"2023-03-15T11:41:51.769195Z","iopub.status.idle":"2023-03-15T11:41:55.499854Z","shell.execute_reply.started":"2023-03-15T11:41:51.769160Z","shell.execute_reply":"2023-03-15T11:41:55.498823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{}},{"cell_type":"code","source":"# Construct train and valid datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/train\", tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:55.501222Z","iopub.execute_input":"2023-03-15T11:41:55.501707Z","iopub.status.idle":"2023-03-15T11:41:56.061650Z","shell.execute_reply.started":"2023-03-15T11:41:55.501667Z","shell.execute_reply":"2023-03-15T11:41:56.060604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start Training","metadata":{}},{"cell_type":"code","source":"# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\ntrain_acc_record = []\ntrain_loss_record = []\nvalid_acc_record = []\nvalid_loss_record = []\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n    \n    train_acc_record.append(train_acc.to('cpu'))\n    train_loss_record.append(train_loss)\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n    \n    valid_acc_record.append(valid_acc.to('cpu'))\n    valid_loss_record.append(valid_loss)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            break","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:41:56.066681Z","iopub.execute_input":"2023-03-15T11:41:56.069026Z","iopub.status.idle":"2023-03-15T11:49:38.426327Z","shell.execute_reply.started":"2023-03-15T11:41:56.068989Z","shell.execute_reply":"2023-03-15T11:49:38.424688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(train_acc_record)\nprint(valid_acc_record)\nprint(train_loss_record)\nprint(valid_loss_record)\n\n\nplt.plot([*range(1,len(train_acc_record)+1)] , train_acc_record , label = \"training\")\nplt.plot([*range(1,len(train_acc_record)+1)] , valid_acc_record , label = \"validation\")\n\nplt.xticks(np.arange(0, n_epochs+1, 5))\nplt.legend(loc=\"upper left\")\n\nplt.savefig('acc.png')\nplt.show()\n\n\nplt.plot([*range(1,len(train_acc_record)+1)] , train_loss_record , label = \"training\")\nplt.plot([*range(1,len(train_acc_record)+1)] , valid_loss_record , label = \"valiidation\")\n\nplt.xticks(np.arange(0, n_epochs+1, 5))\nplt.legend(loc=\"upper left\")\n\nplt.savefig('loss.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:50:00.043149Z","iopub.execute_input":"2023-03-15T11:50:00.043695Z","iopub.status.idle":"2023-03-15T11:50:00.541729Z","shell.execute_reply.started":"2023-03-15T11:50:00.043650Z","shell.execute_reply":"2023-03-15T11:50:00.540641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader for test","metadata":{}},{"cell_type":"code","source":"# Construct test datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntest_set = FoodDataset_TTA(\"/kaggle/input/ml2023spring-hw3/test\", train_tfm , test_tfm , TTA_num)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:49:38.447388Z","iopub.status.idle":"2023-03-15T11:49:38.448328Z","shell.execute_reply.started":"2023-03-15T11:49:38.448000Z","shell.execute_reply":"2023-03-15T11:49:38.448030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and generate prediction CSV","metadata":{}},{"cell_type":"code","source":"model_best = Classifier().to(device)\nmodel_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\nmodel_best.eval()\nprediction = []\nwith torch.no_grad():\n    for train_im , test_im ,_ in tqdm(test_loader):\n        pred = model_best(test_im.to(device)) * TTA_ratio\n        \n        for im in train_im:\n            pred += model_best(im.to(device)) *  (1 - TTA_ratio ) / TTA_num\n        \n        test_label = np.argmax(pred.cpu().data.numpy(), axis=1)\n        prediction += test_label.squeeze().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:49:38.449947Z","iopub.status.idle":"2023-03-15T11:49:38.451281Z","shell.execute_reply.started":"2023-03-15T11:49:38.451011Z","shell.execute_reply":"2023-03-15T11:49:38.451040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(len(test_set))]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:49:38.452643Z","iopub.status.idle":"2023-03-15T11:49:38.453761Z","shell.execute_reply.started":"2023-03-15T11:49:38.453495Z","shell.execute_reply":"2023-03-15T11:49:38.453523Z"},"trusted":true},"execution_count":null,"outputs":[]}]}