{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HW3 Image Classification\n#### Solve image classification with convolutional neural networks(CNN).\n#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com","metadata":{"papermill":{"duration":0.006096,"end_time":"2023-03-26T05:52:36.702354","exception":false,"start_time":"2023-03-26T05:52:36.696258","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Import Packages","metadata":{"papermill":{"duration":0.005102,"end_time":"2023-03-26T05:52:37.839893","exception":false,"start_time":"2023-03-26T05:52:37.834791","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Import necessary packages.\nimport gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"papermill":{"duration":2.797218,"end_time":"2023-03-26T05:52:40.661771","exception":false,"start_time":"2023-03-26T05:52:37.864553","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.539073Z","iopub.execute_input":"2023-03-29T08:23:01.539566Z","iopub.status.idle":"2023-03-29T08:23:01.547629Z","shell.execute_reply.started":"2023-03-29T08:23:01.539516Z","shell.execute_reply":"2023-03-29T08:23:01.546136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myseed = 1091102  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"papermill":{"duration":0.121173,"end_time":"2023-03-26T05:52:40.788833","exception":false,"start_time":"2023-03-26T05:52:40.667660","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.549728Z","iopub.execute_input":"2023-03-29T08:23:01.550065Z","iopub.status.idle":"2023-03-29T08:23:01.567426Z","shell.execute_reply.started":"2023-03-29T08:23:01.550033Z","shell.execute_reply":"2023-03-29T08:23:01.566036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{"papermill":{"duration":0.005109,"end_time":"2023-03-26T05:52:40.799649","exception":false,"start_time":"2023-03-26T05:52:40.794540","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\n\nimage_size = (256,256)\ntest_tfm = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.Resize(image_size),\n    transforms.RandomRotation(45),\n    transforms.RandomCrop((200,200),padding_mode=\"edge\"),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomAdjustSharpness(1.5, p=0.5),\n    transforms.RandomAutocontrast(p=0.5),\n    transforms.RandomPosterize(5, p=0.5),\n    transforms.RandomPerspective(distortion_scale=0.15,p=0.5),\n#     transforms.ElasticTransform(alpha=10.0),\n#     transforms.AugMix(1),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.5,scale=(0.005,0.005),value=(random.random(),random.random(),random.random())),\n    transforms.RandomErasing(p=0.5,scale=(0.005,0.005),value=(random.random(),random.random(),random.random())),\n    transforms.RandomErasing(p=0.5,scale=(0.005,0.005),value=(random.random(),random.random(),random.random())),\n    transforms.RandomErasing(p=0.5,scale=(0.005,0.005),value=(random.random(),random.random(),random.random())),\n])","metadata":{"papermill":{"duration":0.018611,"end_time":"2023-03-26T05:52:40.823460","exception":false,"start_time":"2023-03-26T05:52:40.804849","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.569717Z","iopub.execute_input":"2023-03-29T08:23:01.570753Z","iopub.status.idle":"2023-03-29T08:23:01.582603Z","shell.execute_reply.started":"2023-03-29T08:23:01.570710Z","shell.execute_reply":"2023-03-29T08:23:01.581500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{"papermill":{"duration":0.005077,"end_time":"2023-03-26T05:52:40.833684","exception":false,"start_time":"2023-03-26T05:52:40.828607","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return im,label","metadata":{"papermill":{"duration":0.016103,"end_time":"2023-03-26T05:52:40.855061","exception":false,"start_time":"2023-03-26T05:52:40.838958","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.584831Z","iopub.execute_input":"2023-03-29T08:23:01.585849Z","iopub.status.idle":"2023-03-29T08:23:01.598670Z","shell.execute_reply.started":"2023-03-29T08:23:01.585789Z","shell.execute_reply":"2023-03-29T08:23:01.597286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FoodDataset_TTA(Dataset):\n\n    def __init__(self,path, train_tfm , test_tfm , TTA_num , files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.train_transform = train_tfm\n        self.test_transform = test_tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        train_im = []\n        for i in range(TTA_num):\n             train_im.append(self.train_transform(im))\n            \n        test_im = self.test_transform(im)\n        \n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return train_im,test_im,label","metadata":{"papermill":{"duration":0.017766,"end_time":"2023-03-26T05:52:40.877927","exception":false,"start_time":"2023-03-26T05:52:40.860161","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.602140Z","iopub.execute_input":"2023-03-29T08:23:01.602674Z","iopub.status.idle":"2023-03-29T08:23:01.615971Z","shell.execute_reply.started":"2023-03-29T08:23:01.602622Z","shell.execute_reply":"2023-03-29T08:23:01.614598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{"papermill":{"duration":0.005584,"end_time":"2023-03-26T05:52:40.888654","exception":false,"start_time":"2023-03-26T05:52:40.883070","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n#         self.cnn = models.efficientnet_v2_m()\n        self.cnn = models.resnext101_32x8d()\n        self.fc = nn.Linear(1000 , 11)\n        \n    def forward(self, x):\n        out = self.cnn.conv1(x) \n        out = self.cnn.bn1(out)\n        out = self.cnn.relu(out) \n        out = self.cnn.maxpool(out)\n        out = self.cnn.layer1(out)\n        out = self.cnn.layer2(out)\n        out = self.cnn.layer3(out)\n#         out = self.cnn(x)\n#         out = self.fc(out)\n        return out","metadata":{"papermill":{"duration":0.015887,"end_time":"2023-03-26T05:52:40.910468","exception":false,"start_time":"2023-03-26T05:52:40.894581","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.617883Z","iopub.execute_input":"2023-03-29T08:23:01.619105Z","iopub.status.idle":"2023-03-29T08:23:01.628918Z","shell.execute_reply.started":"2023-03-29T08:23:01.619052Z","shell.execute_reply":"2023-03-29T08:23:01.627729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configurations","metadata":{"papermill":{"duration":0.004976,"end_time":"2023-03-26T05:52:40.921206","exception":false,"start_time":"2023-03-26T05:52:40.916230","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\nbatch_size = 32\nTTA_num = 5\nTTA_ratio = 0.8\ntrain_valid_ratio = 0.9\n","metadata":{"papermill":{"duration":7.113487,"end_time":"2023-03-26T05:52:48.039705","exception":false,"start_time":"2023-03-26T05:52:40.926218","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.630377Z","iopub.execute_input":"2023-03-29T08:23:01.630756Z","iopub.status.idle":"2023-03-29T08:23:01.645460Z","shell.execute_reply.started":"2023-03-29T08:23:01.630722Z","shell.execute_reply":"2023-03-29T08:23:01.643966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{"papermill":{"duration":0.005334,"end_time":"2023-03-26T05:52:48.050435","exception":false,"start_time":"2023-03-26T05:52:48.045101","status":"completed"},"tags":[]}},{"cell_type":"code","source":"files =  [os.path.join(\"/kaggle/input/ml2023spring-hw3/train\",x) for x in os.listdir(\"/kaggle/input/ml2023spring-hw3/train\") if x.endswith(\".jpg\")]\nfiles += [os.path.join(\"/kaggle/input/ml2023spring-hw3/valid\",x) for x in os.listdir(\"/kaggle/input/ml2023spring-hw3/valid\") if x.endswith(\".jpg\")]\nlabel_file = [[] for i in range(11)]\ntrain_file = []\nvalid_file = []\nfor file in files:\n    label = int(file.split(\"/\")[-1].split(\"_\")[0])\n    label_file[label].append(file)\nfor idx , label in enumerate(label_file):\n    random.shuffle(label_file[idx])\n    pick_num =  int(len(label) * train_valid_ratio)\n    \n    for num , file in enumerate(label):\n        if num < pick_num:\n            train_file.append(file)\n        else:\n            valid_file.append(file)\ntrain_file.sort()\nvalid_file.sort()\n\ndel files , label_file\ngc.collect()","metadata":{"papermill":{"duration":0.333647,"end_time":"2023-03-26T05:52:48.389127","exception":false,"start_time":"2023-03-26T05:52:48.055480","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.647196Z","iopub.execute_input":"2023-03-29T08:23:01.647793Z","iopub.status.idle":"2023-03-29T08:23:01.905252Z","shell.execute_reply.started":"2023-03-29T08:23:01.647754Z","shell.execute_reply":"2023-03-29T08:23:01.904043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Construct train and valid datasets.\n# The argument \"loader\" tells how torchvision reads the data.\n\ntrain_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/train\", tfm=train_tfm , files = train_file)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm , files = valid_file)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\nprint(len(train_set))\nprint(len(valid_set))","metadata":{"papermill":{"duration":0.041922,"end_time":"2023-03-26T05:52:48.436496","exception":false,"start_time":"2023-03-26T05:52:48.394574","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-29T08:23:01.908807Z","iopub.execute_input":"2023-03-29T08:23:01.909291Z","iopub.status.idle":"2023-03-29T08:23:01.953371Z","shell.execute_reply.started":"2023-03-29T08:23:01.909242Z","shell.execute_reply":"2023-03-29T08:23:01.951963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# t-SNE","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport torch.nn as nn\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the trained model\nread_model = '/kaggle/input/models/sample_best.ckpt'\nmodel = Classifier().to(device)\nstate_dict = torch.load(read_model , map_location=torch.device('cpu'))\nmodel.load_state_dict(state_dict)\nmodel.eval()\n# del state_dict\n# gc.collect()\nprint(\"!\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:23:01.956363Z","iopub.execute_input":"2023-03-29T08:23:01.956744Z","iopub.status.idle":"2023-03-29T08:23:04.435236Z","shell.execute_reply.started":"2023-03-29T08:23:01.956708Z","shell.execute_reply":"2023-03-29T08:23:04.433926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model.cnn)\n\n# new_model = nn.Sequential(model.cnn.conv1 , \n#                           model.cnn.bn1 ,\n#                           model.cnn.relu ,\n#                         model.cnn.maxpool,\n#                           model.cnn.layer1\n#                          )\n# new_model.eval()\n# print(\"!\")","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:23:04.436889Z","iopub.execute_input":"2023-03-29T08:23:04.437721Z","iopub.status.idle":"2023-03-29T08:23:04.443863Z","shell.execute_reply.started":"2023-03-29T08:23:04.437668Z","shell.execute_reply":"2023-03-29T08:23:04.442506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del model , train_set , train_loader\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:23:04.445450Z","iopub.execute_input":"2023-03-29T08:23:04.446655Z","iopub.status.idle":"2023-03-29T08:23:04.453516Z","shell.execute_reply.started":"2023-03-29T08:23:04.446615Z","shell.execute_reply":"2023-03-29T08:23:04.452652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index = 11 # You should find out the index of layer which is defined as \"top\" or 'mid' layer of your model.\nfeatures = []\nlabels = []\nfor batch in tqdm(valid_loader):\n    imgs, lbls = batch\n    with torch.no_grad():\n        logits = model(imgs.to(device))\n        logits = logits.view(logits.size()[0], -1)\n    labels.extend(lbls.cpu().numpy())\n    logits = np.squeeze(logits.cpu().numpy())\n    features.extend(logits)\n    \n    \nfeatures = np.array(features)\ncolors_per_class = cm.rainbow(np.linspace(0, 1, 11))\n\nprint(\"T-SNE\")\ndel valid_set , valid_loader\ngc.collect()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:23:04.454666Z","iopub.execute_input":"2023-03-29T08:23:04.455795Z","iopub.status.idle":"2023-03-29T08:29:23.011813Z","shell.execute_reply.started":"2023-03-29T08:23:04.455759Z","shell.execute_reply":"2023-03-29T08:29:23.010606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(features.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:29:23.013649Z","iopub.execute_input":"2023-03-29T08:29:23.014680Z","iopub.status.idle":"2023-03-29T08:29:23.020638Z","shell.execute_reply.started":"2023-03-29T08:29:23.014625Z","shell.execute_reply":"2023-03-29T08:29:23.019334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply t-SNE to the features\nfeatures_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features)\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(10, 8))\nfor label in np.unique(labels):\n    plt.scatter(features_tsne[labels == label, 0], features_tsne[labels == label, 1], label=label, s=5)\nplt.legend()\nplt.savefig(\"t_sne.jpg\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-29T08:29:23.022467Z","iopub.execute_input":"2023-03-29T08:29:23.022896Z","iopub.status.idle":"2023-03-29T08:30:56.535151Z","shell.execute_reply.started":"2023-03-29T08:29:23.022858Z","shell.execute_reply":"2023-03-29T08:30:56.533775Z"},"trusted":true},"execution_count":null,"outputs":[]}]}